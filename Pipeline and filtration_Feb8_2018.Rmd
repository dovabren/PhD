---
title: "R Notebook"
output: html_notebook
---
pipeline
steps gone through with Tanushree:
*example is all on Q11D1, see Loop_projectX for more than one file*
log into apis server
copy of raw data remade (then found fastq file that was already remade)
everyone works off of the media file in apis
Erica's data is in here, too
need to check the space -h/media/data1-- cannot exceed the space or it will crash, can copy data from apis to bombus before you run -- makes temporary files so the size of your file multiplied by at least 4 is how much space you need 


```{bash}
cd/media/data1/projectx/ fastq #there are queen (Q11) and drone (Q11D) data here

#check space in server
df -h/media/data1
```
move to bombus server 
log into other server 

to stop any process in the middle do CTRL+C

now log into bombus server 
```{bash}
#my file to use
dova_projectX_analysis
```

first run trimmomatic command before the filtration- filter for low quality and adapter content 
given options like java-jar Trimmomatic.jar PE (parents) 
R1 and R2 are fwd and the reverse of the same thing
threads- they do them in parallel -48 threads on each server so do around 36 (not more than 40 bc the threads are being used)

Trimmomatic:
```{bash}
java -jar /data4/Trimmomatic-0.36/trimmomatic-0.36.jar PE -threads 36 -phred33 HI.2178.008.Index_13.Q11D1_R1.fastq  HI.2178.008.Index_13.Q11D1_R2.fastq  Q11D1_paired_R1.fastq.gz  Q11D1_unpaired_R1 .fastq.gz  Q11D1_paired_R2.fastq.gz  Q11D1_unpaired_R2.fastq.gz ILLUMINACLIP :/usr/local/scripts/ADAPTER.fa:2:30:10 LEADING:20 TRAILING:20 SLIDINGWINDOW:20:25 MINLEN:35

```
Parameters:
¬	PE : Paired End
¬	threads : number of threads
¬	PHRED33: Convert quality scores to Phred-33
¬	ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read
¬	ADAPTER.fa : fasta sequences for universal Illumina adapter and index sequences from Illumina adapter pdf file. The contents of adapter file are there in supplementary data S1
¬	LEADING:<quality>: Specifies the minimum quality required to keep a base
¬	TRAILING:<quality> : Specifies the minimum quality required to keep a base
¬	SLIDINGWINDOW:<windowSize>:<requiredQuality>
¬	windowSize: specifies the number of bases to average across
¬	requiredQuality: specifies the average quality required.
¬	MINLEN:<length>: Specifies the minimum length of reads to be kept


will make 4 files for each sample- sometimes wont have R1 and R2 so the leftovers go to an unpaired file (we only keep the ones that are high quality and that they have adapters, they are not yet mapped)
need to find adapter and if they are on both R1 and R2 then trim them a bit 

high quality- from 100bp reads, we want minimum 35 bp bc the adapter around 20 so minumum is 35 bp so can get some good quality there (if 150bp reads, then look for minimum of 50bp length)

next you will get the paired read and then do the FastQC- look at the quality of the data and see if its acceptable or if you want more strict filtering applied to it 
will get a graphical output that tells you what it looks like

then can do other analysis -- map the reads to the reference genome apis mellifera and then NGM tool and give it the options 


FastQC:
```{bash}
locate am45new #this is the reference genome in fasta format
#will tell you what its called on the bombus server 
NGM -r

#command
fastqc –t 36 *paired*

```
¬	Input : Filtered fastq files
¬	-t/–threads: Specifies the number of files which can be processed                     simultaneously.  Each thread will be allocated 250MB of memory 

Note: The following QC indicators should not have the “FAIL” Red Cross in the FastQC report.
¬	 a) adapter_content
¬	 b) sequence_duplication_levels 
¬	 c) per_base_sequence_quality
¬	 d) per_sequence_gc_content 
¬	 e) overrepresented_sequences 
¬	 f) GC content range (29 min - 37 max)

-p paired reads 
-1 R1
-2 R2
then how many threads to use
-b for bam file 


check the location of the program, let it autocomplete if its there so then have to look up the path 

htop to see what else is going on- they should mostly be blank
make sure nothing else is running so that you wont break theres or your prgams

output files are the HI.2178.008_______R1 and then the _______R2 

have to run the drones individually 

can do 8 samples in a folder and it will execute the command if you run a loop (look up for later)

each sample takes about 2 hours to run 

see what the file looks like 
unpaired read file size is always smaller than the paired -double check
```{bash}
fastqc -t 36 *paired* #only get the files with paried in it 
```
when complete it will make you a file HTML file - copy them to local and look at them 
first is a basic stats - how many reads, avg length, etc
GC%

average should be higher than 90 since ours is 100bp

when looking at the summary, there shouldnt be failures (red X and orange !)
see file Tanu sends for possible errors and what to look for 

NGM : Filtered read alignment to the reference genome:
```{bash}
ngm -r /data2/Training_set/am45new.fasta -p -1 Q11D1_paired_R1.fastq -2 Q11D1_paired_R2.fastq -t 36  -b --rg-id Q11D1 --rg-sm Q11D1 --rg-lb PE --rg-pl Illumina -o Q11D1_ngm_aligned.bam
```

¬	-r : Reference genome sequence, the index file foe genome is also required in the same folder as of reference
¬	-p : paired end reads, -1 (first pair of read i.e. R1) and -2 (second pair of read i.e. R2)
¬	-t : number of threads
¬	-b : output the bam file
¬	--rg-id <string> : Adds RG:Z:<string> to all alignments in SAM/BAM
¬	 --rg-sm<string> : RG header: Sample
¬	 --rg-lb <string> : RG header: Library
¬	--rg-pl <string> : RG header: Platform
¬	 -o : output file
Note: The RG header things are required for downstream processing og bam files


downstream analysis with NGM aligner - maps to ref genome and produce a bam file
(can combine many files later, now only have 4 drones)
NGM will do the mapping

rg-id for bam file, needs sample name after it Q11D1

after command entered for NGM, will give stats of how many were mapped to the reference genome - need this info before joining files together so you know the max stats of your data 

output is a bam file - will make 3 versions of a bam file 
the bam file has all info that was present in the fastq file - which reads were mapped - need to sort it before we analyze it further 

Samtools:
```{bash}
samtools sort Q11D1_ngm_aligned.bam -o Q11D1_ngm_aligned_sorted.bam 
samtools index Q11D1_ngm_aligned_sorted.bam
```

sort so all unread files are at the bottom and all read are at the top-- sort with SAM tools -- ask it to sort the bam file we just made

samtools sort old_name.bam new_name.bam
samtool index new_name.bam

Picard : marking  duplicate reads
```{bash}
java -jar /usr/local/src/picard-tools-2.1.0/picard.jar MarkDuplicates I=Q11D1_ngm_aligned_sorted.bam O=Q11D1_marked_duplicates.bam M= Q11D1_marked_dup_metrics.txt VALIDATION_STRINGENCY=SILENT MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=1000


samtools index Q11D1_marked_duplicates.bam
```

Parameters:
¬	I: Ngm aligned and samtools sorted bam file
¬	O: Duplicate marked bam file
¬	M: metrics report of duplicate mapping

next is picard to mark duplicate reads- sometimes in genome, there are too many reads so remove duplicate reads that might get mapped to different places or if sequenced too many times (also a java program)

do picard on NGM sorted aligned file 
say we dont want any DNA fragments, read cut off is it 1000- 100% similar , 1000 times then discard it 

how many reads, how many times etc is put into a _metrics file 

reindex bc some files are now discarded so reindex- dont need to sort the whole thing 

--finish until here and practice - what does everything mean, how do we get the stats of the bam files (SAM tools)- how many reads are properly saved
parameter in sam tools 

```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T BaseRecalibrator -nct 36 -R /data2/Training_set/am45new.fasta -I Q11D1_marked_duplicates.bam --knownSites /data2/Training_set/INDEL_TRAINING.vcf --knownSites /data2/Training_set/SNP_TRAINING.vcf -o Q11D1.recal.table
```



```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T PrintReads -R /data2/Training_set/am45new.fasta -nct 36 -I Q11D1_marked_duplicates.bam -BQSR Q11D1.recal.table -o Q11D1_BQSR.bam
```

*STOP HERE IF USING KATIE"S HAPLOTYPE CALLER*"

the GVCF command is going to insert the reference genome where ever there is missing data or information 
make this GVCF then merge the GVCF and make just a VCF that will be fed into filtration 
```{bash}
#old haplotype caller 
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -R /data2/Training_set/am45new.fasta -I $sid"_BQSR.bam" --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 -o $sid"_raw_variants.g.vcf"

#new- for each drone
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 40 -R /data2/Training_set/am45new.fasta -I Q11D1_BQSR.bam --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 --emitRefConfidence GVCF -ploidy 1 -o Q11D1_raw_variants.g.vcf

#new- queen
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 40 -R /data2/Training_set/am45new.fasta -I Q11_BQSR.bam --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 --emitRefConfidence GVCF -ploidy 2 -o Q11_raw_variants.g.vcf
```

 run the command above on all the drones BQSR bams to produce their gvcf files, then you have to merge the gvcf files using this command
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -R /data2/Training_set/am45new.fasta --variant Q11D1_raw_variants.g.vcf --variant Q11D2_raw_variants.g.vcf --variant Q11D3_raw_variants.g.vcf --variant Q11D4_raw_variants.g.vcf --variant Q11D5_raw_variants.g.vcf -o drones_1to5.g.vcf 
```

*katie's haplotype caller*
```{bash}
#use 1 for haploid, 2 for diploid
gatk  -T HaplotypeCaller -nct 20 -R reference.fa -I File.Bam -emitRefConfidence GVCF -variant_index_type LINEAR -variant_index_parameter 128000 -ploidy 2 -o OutFile.snps.indels.g.vcf 

#not sure when to use this
gatk -T CombineGVCFs -R reference.fa --variant --variant --variant ... -o cohort.g.vcf
```

the join genotype is going to combine many GVCFs together when working with many (100+) drones 
at this point will need to filter based on chromosome (ASK HOW TO SPLIT THIS UP) bc it will otherwise take too long so split to 1000 reads at a time 

this is only going to call alternate alleles - if there is no data present it might think this is a mistake 

can tell it that it is haploid or diploid with the -ploidy 2 or 1 commans -->this step is instead of using the heterozygous SNP filter 

convert g.vcf into vcf
```{bash}
#from katie, run in parallel windows
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta -V sample1.g.vcf -V sample2.g.vcf -V sample3.g.vcf ...ect -o output.final.vcf

#run one at a time 
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta -V Q11D1_raw_variants.g.vcf -o Q11D1_raw_variants_final.vcf
```

merge
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta --variant Q11D1_raw_variants.g.vcf --variant Q11D2_raw_variants.g.vcf --variant Q11D4_raw_variants.g.vcf --variant Q11D5_raw_variants.g.vcf -o drones_1to5.g.vcf
```


this is where the loop will be run (command is "bash" bc even though it is a text file we want it to be read as bash commands and not just text)
can check on the status of the loop by using "ls" to ensure that new files are being made or by using "htop" to see the progress, 

run a second screen- this will run on the server so that if i close my laptop it will still work (not being run off of my machine)
```{bash}
screen -S name -L
bash Loop_ProjectX
bash queen-pipeline
```

for loop: dont need quotes around names if not in loop


*Dec 24*
downstream filtration cleaned 
after using g.vcf
keep only indels- this will be used as a reference for later with GATK
(continue from erica_pipeline)

```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_1to5.g.vcf -selectType INDEL -o drones_1to5_indels.vcf
```
time to complete: immediately 

keep only SNPs
**still do this for queen**
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_1to5.g.vcf -selectType SNP -o drones_1to5_snp.vcf
```
time to complete: immediately 

GATK variant filtrtion
check sorting of each file -- harshil said this will sort well 
 
```{bash}
#dont need to use this one bc it sorted properly
python3 dova-pipeline-tools.py sort_vcf  <input.vcf> <output.vcf>
```


5bp indel
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_1to5_snp.vcf -mask drones_1to5_indels.vcf --maskExtension 5 --maskName "bad_snp" -o drones_1to5_snps_filter1.vcf 
```
time to complete: immediately 

After the filter step above with gatk variant filtration 
you need to change the version of the vcf with:
*still do this for queen*
```{bash}
cat drones_1to5_snps_filter1.vcf | vcf-convert -v 4.1 > drones_1to5_snps_filter2.vcf 
```
time to complete: about a minute 

so the filter only labels bad snps it doesnt remove them so you have to run this line to remove them out
*still do this for queen*
```{bash}
vcftools --vcf drones_1to5_snps_filter2.vcf --remove-filtered-all --recode --recode-INFO-all --out drones_1to5_snps_filter3.vcf 
```
time to complete: immediately 

output here is: drones_1to5_snps_filter3.vcf.log  drones_1to5_snps_filter3.vcf.recode.vcf

#Remove SNPs from unplaced scaffolds
this is chromosome 17 and 18
```{bash}
grep -v '^17\.' drones_1to5_snps_filter3.vcf.recode.vcf > drones_1to5_snp_filter4.vcf

#ask if should remove 18 bc its mitochondrial 
grep -v '^18\.' drones_1to5_snp_filter4.vcf > drones_1to5_snp_filter5.vcf 
```

#Remove SNPs with lowest 10% quality score - find minimum phred quality score (hers was 140)
awk {'print $6'} 
number at the end is the quality you are filtering on 
```{bash}
python3 dova-pipeline-tools.py filter_qual drones_1to5_snp_filter5.vcf drones_1to5_snp_filter6.vcf 10

```


#Filter of average depth (12 to 88 reads)-- need to multiply by 4 bc merged 4 drone files (changed this in harshil's python code stored on server)

```{bash}
python3 dova-pipeline-tools.py filter_depth drones_1to5_snp_filter6.vcf drones_1to5_snp_filter7.vcf 48 352
```


redo haplotype caller with -ploidy 2
6 hour running time 
```{bash}
#dont use this -feb 8,2018
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 40 -R /data2/Training_set/am45new.fasta -I Q11_BQSR.bam --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 --emitRefConfidence GVCF -ploidy 2 -o Q11_raw_variants.g.vcf
```
ignore above

use:
gatk --java-options "-Xmx4g" HaplotypeCaller  \
   -R Homo_sapiens_assembly38.fasta \
   -I input.bam \
   -O output.g.vcf.gz \
   -ERC GVCF
run on bqsr.bam
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -R /data2/Training_set/am45new.fasta -I Q11_BQSR.bam -o Q11_raw_variants.g.vcf.gz -ERC GVCF
```

run this on 2 samples, try to merge 

old--> merge with nothing bc only one file- still need this step 
7 min running time 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta --variant Q11_raw_variants.g.vcf -o Q11_raw_variants2.g.vcf
```

make INDEL file 
2 min running time 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V Q11_raw_variants2.g.vcf -selectType INDEL -o Q11_indels.vcf
```

make SNP file 
2 min running time 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V Q11_raw_variants2.g.vcf -selectType SNP -o Q11_snp.vcf
```

Variant filtration to mask bad SNP
1 min 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V Q11_snp.vcf -mask Q11_indels.vcf --maskExtension 5 --maskName "bad_snp" -o queen_filter1.vcf
```

version change
```{bash}
cat queen_filter1.vcf | vcf-convert -v 4.1 > queen_filter2.vcf
```

 filter only labels bad snps it doesnt remove them so you have to run this line to remove them out
```{bash}
vcftools --vcf queen_filter2.vcf --remove-filtered-all --recode --recode-INFO-all --out queen_filter3.vcf
```

Remove SNPs from unplaced scaffolds-- this is chromosome 17 and 18
```{bash}
grep -v '^17\.' queen_filter3.vcf.recode.vcf > queen_filter4.vcf 

grep -v '^18\.' queen_filter4.vcf > queen_filter5.vcf
```

Remove SNPs with lowest 10% quality score - find minimum phred quality score (hers was 140)
```{bash}
python3 dova-pipeline-tools.py filter_qual queen_filter5.vcf queen_filter6.vcf 10
```

Filter of average depth (12 to 88 reads) 
```{bash}
python3 dova-pipeline-tools.py filter_depth queen_filter6.vcf queen_filter7.vcf 12 88
```


---------------------------------------------------------------------------------------------------

*feb 5, 2018*

*re-ran samtools sort on Q11_BQSR.bam and then re-ran the haplotype caller WITHOUT the -ploidy 2 in order to merge with the drone files (cannot merge -ploidy 1 and -ploidy 2)*
start: 12:23pm
end: 
```{bash}
samtools index Q11_BQSR.bam

java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 40 -R /data2/Training_set/am45new.fasta -I Q11_BQSR.bam --genotyping_mode DISCOVERY -stand_emit_conf 10 -stand_call_conf 30 --emitRefConfidence GVCF -o Q11_raw_variants_no_p.g.vcf
```


merge queen and drone g.vcfs 
```{r setup, include=FALSE}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta --variant Q11D1_raw_variants.g.vcf --variant Q11D2_raw_variants.g.vcf --variant Q11D4_raw_variants.g.vcf --variant Q11D5_raw_variants.g.vcf --variant Q11_raw_variants_no_p.g.vcf -o drones_queen.g.vcf
```


make INDEL file 
2 min running time 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_queen.g.vcf -selectType INDEL -o drones_queen_indels.vcf
```

make SNP file 
2 min running time 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_queen.g.vcf -selectType SNP -o drones_queen_snp.vcf
```

Variant filtration to mask bad SNP
1 min 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_snp.vcf -mask drones_queen_indels.vcf --maskExtension 5 --maskName "bad_snp" -o drones_queen_filter1.vcf
```
for ambiguous sites - 

version change
```{bash}
cat drones_queen_filter1.vcf | vcf-convert -v 4.1 > drones_queen_filter2.vcf
```

 filter only labels bad snps it doesnt remove them so you have to run this line to remove them out
```{bash}
vcftools --vcf drones_queen_filter2.vcf --remove-filtered-all --recode --recode-INFO-all --out drones_queen_filter3.vcf
```

Remove SNPs from unplaced scaffolds-- this is chromosome 17 and 18
```{bash}
grep -v '^17\.' drones_queen_filter3.vcf.recode.vcf > drones_queen_filter4.vcf 

grep -v '^18\.' drones_queen_filter4.vcf > drones_queen_filter5.vcf 
```

*Harshil's code puts together the depth and quality, can specify parameters*

```{bash}
python3 /data3/dova_projectX_analysis/dec13test/dova-pipeline-tools.py drones_queen_filter5.vcf drones_queen_filter6.vcf 10 
```




Remove SNPs with lowest 10% quality score - find minimum phred quality score (hers was 140)
```{bash}
python3 /data3/dova-pipeline-tools.py filter drones_queen_filter5.vcf drones_queen_filter6.vcf qual 10 90
```

Filter of average depth (12 to 88 reads) *ask if need to x5*
*when merged queen and drone use 1.5 IQR which is 53 and 168
```{bash}
python3 /data3/dova-pipeline-tools.py filter drones_queen_filter6.vcf drones_queen_filter7.vcf dep
```
how the depth filter works: (instead of IQR)
1. orders data from largest to smaller
2. measures distance to previous neighbour
3. the difference increases when they become outlier
4. second derivative (repeat process for the differences)- when it starts changing by a lot is the new filter -- can graph this process - see spike in outliers

argmax- gives index where the data starts to be very different

double sort the data- once before you apply the quality filter and then again after get the first derivative to get the second derivative to know where the cut off should be and where the outliers really begin 

*finding the mutation rate*
00 is homozygous for the reference
1/1 is 2 alternate alleles 
mutation rate: split the queen on the slash, make a list of queen 1/0 or 1/1 or 0/1 or 0/0- if any drones do not match the queen, then add it 
if the queen has a . instead of a number, it isnt included (not assuming if its a mutation or not  a mutation, just excluded)

mutation rate- how erica did it 
mutation rate 2- using the whole genome as the denominator 

include the fact that queen is diploid and drones are haploid so need to divide by 2 

---------------------------------------------------------------------------------------------------
*feb 7,2018*
katie said not to use the strict criteria of 10% quality and depth (even though calculated)
use GATK: https://gatkforums.broadinstitute.org/gatk/discussion/6925/understanding-and-adapting-the-generic-hard-filtering-recommendations

use all of these instead- dont need to be in any particular order 

---------------------------------------------------------------------------------------------------*feb 8, 2018*
tanushree:
#1. yes, use GATK hard filters instead
#2. try to remerge the queen and drone using:

gatk --java-options "-Xmx4g" HaplotypeCaller  \
   -R Homo_sapiens_assembly38.fasta \
   -I input.bam \
   -O output.g.vcf.gz \
   -ERC GVCF
run on bqsr.bam
*feb 7-10*
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 40 -R /data2/Training_set/am45new.fasta -I Q11_BQSR.bam -o Q11_raw_variants.g.vcf.gz -ERC GVCF

java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 24 -R /data2/Training_set/am45new.fasta -I Q11D1_BQSR.bam -o Q11D1_raw_variants.g.vcf.gz -ERC GVCF

java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 24 -R /data2/Training_set/am45new.fasta -I Q11D2_BQSR.bam -o Q11D2_raw_variants.g.vcf.gz -ERC GVCF

java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 24 -R /data2/Training_set/am45new.fasta -I Q11D4_BQSR.bam -o Q11D4_raw_variants.g.vcf.gz -ERC GVCF

java -jar /usr/local/src/GenomeAnalysisTK.jar -T HaplotypeCaller -nct 24 -R /data2/Training_set/am45new.fasta -I Q11D5_BQSR.bam -o Q11D5_raw_variants.g.vcf.gz -ERC GVCF
```

*feb 10*
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T GenotypeGVCFs -R /data2/Training_set/am45new.fasta --variant Q11_raw_variants.g.vcf.gz --variant Q11D1_raw_variants.g.vcf.gz --variant Q11D2_raw_variants.g.vcf.gz --variant Q11D4_raw_variants.g.vcf.gz --variant Q11D5_raw_variants.g.vcf.gz -o drones_queen.g.vcf
```

*feb 11*
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_queen.g.vcf -selectType INDEL -o drones_queen_indels.vcf

java -jar /usr/local/src/GenomeAnalysisTK.jar -T SelectVariants -R /data2/Training_set/am45new.fasta -V drones_queen.g.vcf -selectType SNP -o drones_queen_snp.vcf

```

recalibration -*feb 26/27*
java -Xmx4g -jar GenomeAnalysisTK.jar \
   -T VariantRecalibrator \
   -R /data2/Training_set/am45new.fasta
   -input drones_queen.g.vcf \ 
   -recalFile path/to/output.recal \ (new output, name myself)
   -tranchesFile path/to/output.tranches \ (new output, name myself)
   -nt 4 \
    (training set)
   -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP -an InbreedingCoeff
   -mode SNP
   
SNP specific recommendations
    -resource:hapmap,known=false,training=true,truth=true,prior=15.0 Apis_mellifera_DBSNPs_R1.vcf 
   -resource:Drone,known=false,training=true,truth=false,prior=10.0 Drone_Filter1_Filter2_R1.vcf 
   -resource:dbsnp,known=true,training=false,truth=false,prior=12.0 combined_snps.vcf \
   -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP -an InbreedingCoeff \
   
   
   -mode SNP \ -mode INDEL
   
Indel specific recommendations
--maxGaussians 4 \
   -resource:mills,known=false,training=true,truth=true,prior=12.0 INDEL_TRAINING.vcf \
   -an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum -an InbreedingCoeff \
   -mode INDEL \
```{bash}
#Feb 27- run on merged queen and drones, made with -ERC GVCF
#Recalibration SNPs
java -Xmx4g -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantRecalibrator -R /data2/Training_set/am45new.fasta -input drones_queen.g.vcf -recalFile drones_queen.recal -tranchesFile drones_queen.tranches -resource:hapmap,known=false,training=true,truth=true,prior=15.0 /data2/Training_set/SNP_TRAINING.vcf -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an SOR -an DP -mode SNP -nt 4

#Tanushree merged the different training sets so now only need to use one called SNP_TRAINING.vcf
#will also get a mean, median etc for each annotation- save these and use as a comparison after the hard filters are done to make sure that they are actually changing 


#Recalibration INDEL -NOT DONE FEB 27, needed for variant filtration 
java -Xmx4g -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantRecalibrator -R /data2/Training_set/am45new.fasta -input drones_queen.g.vcf -resource:Drone,known=true,training=true,truth=true,prior=12.0 /data2/Training_set/INDEL_TRAINING.vcf -an QD -an DP -an FS -an SOR -an ReadPosRankSum -an MQRankSum -mode INDEL --maxGaussians 4 -recalFile drones_queen_indel.recal -tranchesFile drones_queen_indel.tranches -nt 4
```

 

*Feb 27*
result of SNP recalibration:
INFO  17:07:20,444 VariantDataManager - QD:      mean = 25.99    standard deviation = 6.17
INFO  17:07:20,608 VariantDataManager - MQ:      mean = 59.44    standard deviation = 3.29
INFO  17:07:20,754 VariantDataManager - MQRankSum:       mean = 0.04     standard deviation = 0.96
INFO  17:07:20,917 VariantDataManager - ReadPosRankSum:          mean = 0.15     standard deviation = 1.20
INFO  17:07:21,081 VariantDataManager - FS:      mean = 2.39     standard deviation = 4.88
INFO  17:07:21,206 VariantDataManager - SOR:     mean = 0.86     standard deviation = 0.39
INFO  17:07:21,331 VariantDataManager - DP:      mean = 240.93   standard deviation = 295.18
INFO  17:07:22,499 VariantDataManager - Annotations are now ordered by their information content: [DP, MQ, QD, FS, SOR, ReadPosRankSum, MQRankSum]
INFO  17:07:22,558 VariantDataManager - Training with 790796 variants after standard deviation thresholding.

Apply recalibration
java -Xmx3g -jar GenomeAnalysisTK.jar \
   -T ApplyRecalibration \
   -R reference/human_g1k_v37.fasta \
   -input raw.input.vcf \
   -tranchesFile path/to/input.tranches \
   -recalFile path/to/input.recal \
   -o path/to/output.recalibrated.filtered.vcf \
   [SPECIFY THE DESIRED LEVEL OF SENSITIVITY TO TRUTH SITES] \
   [SPECIFY WHICH CLASS OF VARIATION WAS MODELED] \

 --ts_filter_level 99.5 -mode SNP 
 --ts_filter_level 99.0 -mode INDEL
```{bash}
#apply recalibration SNP
java -Xmx3g -jar /usr/local/src/GenomeAnalysisTK.jar -T ApplyRecalibration -R /data2/Training_set/am45new.fasta -input drones_queen.g.vcf -tranchesFile drones_queen.tranches -recalFile drones_queen.recal -o drones_queen_recalibrated_snp.vcf --ts_filter_level 99.5 -mode SNP

#apply recalibration INDEL -NOT DONE FEB 28
java -Xmx3g -jar /usr/local/src/GenomeAnalysisTK.jar -T ApplyRecalibration -R /data2/Training_set/am45new.fasta -input drones_queen.g.vcf -tranchesFile drones_queen_indel.tranches -recalFile drones_queen_indel.recal -o drones_queen_recalibrated_indel.vcf --ts_filter_level 99.0 -mode INDEL
```


#redo feb 27- after recalibration 
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_recalibrated_snp.vcf -mask drones_queen_recalibrated_indel.vcf --maskExtension 5 --maskName "bad_snp" -o drones_queen_snp_filter1.vcf 

cat drones_queen_snp_filter1.vcf | vcf-convert -v 4.1 > drones_queen_snp_filter2.vcf 

vcftools --vcf drones_queen_snp_filter2.vcf --remove-filtered-all --recode --recode-INFO-all --out drones_queen_snp_filter3.vcf

grep -v '^17\.' drones_queen_snp_filter3.vcf.recode.vcf > drones_queen_snp_filter4.vcf

#ask if should remove 18 bc its mitochondrial 
grep -v '^18\.' drones_queen_snp_filter4.vcf > drones_queen_snp_filter5.vcf 
```

*stopped here feb 11*
---------------------------------------------------------------------------------------------------
*Feb 15*

*Hard Filters*
evaluate drones_queen_snp_filter5.vcf to see qual and allele depth to get QD and know where lower cut off is 
1. QualitybyDepth (QD)
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R  /data2/Training_set/am45new.fasta -V drones_queen_snp_filter5.vcf --filterExpression "QD < 2.0" --filterName "LowQD5" -o drones_queen_h_QD_2.vcf
```
-try <2.0 and <5.0
-GATK recommendation >2.0

2. FisherStrand(FS)
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_h_QD_2.vcf --filterExpression "FS > 60.0" --filterName "FS8" -o drones_queen_h_FS_60.vcf
```
-GATK recommendation >60
(see van der Auwera et al., 2014)

3. StrandOddsRatio (SOR)
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_h_FS_60.vcf --filterExpression "SOR > 3.0" --filterName "SOR" -o drones_queen_h_SOR_3.vcf
```
GATK recommendation: >3

4.RMSMappingQuality (MQ)
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_h_SOR_3.vcf --filterExpression "MQ < 40.0" --filterName "LowMQ40" -o drones_queen_h_MQ_40.vcf
```
GATK recommendation: <40

-->  feb 23 --> 5.MappingQualityRankSumTest (MQRankSum)
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_h_MQ_40.vcf --filterExpression "MQRankSum < -2.0" --filterName "MQRSmin5" --filterExpression "MQRankSum > 2.0" --filterName "MQRSmax5" -o drones_queen_h_MQRankSum_2.vcf
```
kaite: -2.0 to 2.0 
GATK:-2.5 to 2.5 -- used this 
*6.ReadPosRankSumTest (ReadPosRankSum)*
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_h_MQRankSum_2.vcf --filterExpression "ReadPosRankSum < -2.0" --filterName "RPRSmin5" --filterExpression "ReadPosRankSum > 2.0" --filterName "RPRSmax5" -o drones_queen_h_RPRS_2.vcf
```

*7. depth (cant find on GATK website) **change depth values**
```{bash}
java -jar /usr/local/src/GenomeAnalysisTK.jar -T VariantFiltration -R /data2/Training_set/am45new.fasta -V drones_queen_hard7.vcf --filterExpression "DP > 13502" --filterName "DPhigh"--filterExpression "DP < 6566" --filterName "DPlow" -o drones_queen_hard8.vcf
```

---------------------------------------------------------------------------------------------------
Katie's hard filter notes *Feb 15, 2018*
# - Hard filters
gatk -T VariantFiltration \
-R /data2/african/files/am45new2.fa \
-V Merged.SNP.Filtered.vcf \
--filterExpression "MQ < 40.0" --filterName "LowMQ40" \
--filterExpression "QD < 5.0" --filterName "LowQD5" \
--filterExpression "FS > 10.50" --filterName "FS8" \
--filterExpression "MQRankSum < -2.0" --filterName "MQRSmin5" \
--filterExpression "MQRankSum > 2.0" --filterName "MQRSmax5" \
--filterExpression "ReadPosRankSum < -2.0" --filterName "RPRSmin5" \
--filterExpression "ReadPosRankSum > 2.0" --filterName "RPRSmax5" \
--filterExpression "SOR > 1.5" --filterName "SOR" \
--filterExpression "DP > 13502" --filterName "DPhigh" \
--filterExpression "DP < 6566" --filterName "DPlow" \
-o Merged.SNP.Filtered.Hard.vcf

#Filtering:
- Everything with an INDEL or CNV tag goes.
- Anything with a VQSR and at least 1 hard filter is removed
- Anything with 2 or more hard filters is removed
- Anything with a single VQSR or single hard filters are looked at (probably retained)

---------------------------------------------------------------------------------------------------
#3. when hard filtering is done do hand filtering:
  A.VCF-filtering on quality and depth - use python to find all heterozygous SNPs in the queen and   check that they are all homozygous in drones 
  --remove any heterozygous drone sites --> these are non-allelic sequencing alignment due to:         CNVs, sequencing error, low sequencing quality (Liu, 2016- recombination paper- they lost 22%      of their SNPs, we dont expect to lose more than that)
  B. haploid phasing - this will identify markers and look at SNP linkage information - it will give the genotypes - including true positives and false positives, so need to rule out the false 
  C. multi-copy regions (Ambiguous sites) - looks at regions of the genome where there are multiple copies of a certain gene or region that is repeated. in these regions, it is likely that mapping or alignemnt might be skewed and report that it is a heterozygous site in the drone when really it should have mapped to another copy region and is not heterozygous. so we want to remove these as well - we map them with the ambiguous sites file (Brock's)- use a similar command to the one used to find INDELs (with --mask) but the file we are reading from is the ambiguous sites file- may get unusually high coverage in these regions but its just becasue the sites are getting confused 

